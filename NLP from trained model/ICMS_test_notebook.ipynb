{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Code\n",
    "\n",
    "The purpose of this code is to illustrate the functionality of the classification system described in *\"A Machine Learning Approach to Classifying Construction Cost Documents into the International Construction Measurement Standard\"* by J. I Deza, H. Ihshaish and L. Mahdjoubi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions \n",
    "\n",
    "1. Run the first Cell of this Jupyter notebook\n",
    "2. Edit the BoQ_text string (some examples are provided)\n",
    "3. Run predict on that string.\n",
    "4. A dictionary containing the ICMS number and it's definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ignacio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ignacio/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ignacio/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/ignacio/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ignacio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re,joblib  \n",
    "import pandas as pd\n",
    "## for processing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\") + ['x', 'mm', 'cm', 'ref', 'wb', 'xx', 'per', 'm']\n",
    "max_features = 6000\n",
    "accepted_words = joblib.load('accepted_words.joblib')\n",
    "clf = joblib.load('Random_Forests_trained_model.joblib.gz')\n",
    "icms_dct = joblib.load('icms1_dictionnary.joblib') \n",
    "Cat = joblib.load('Categories_ML.joblib')\n",
    "\n",
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    #remove if only two characters\n",
    "    text_big = re.sub(r'\\W*\\b\\w{1,2}\\b', '', text) \n",
    "          \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text_big.split()\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in lst_stopwords]\n",
    "    \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "\n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "                 ## removing tags\n",
    "        ## removing digits\n",
    "\n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "\n",
    "def clean_text_data(df,col):\n",
    "    # Separate data\n",
    "    desc_lower = df[col]\n",
    "    # Remove text before \"|\" character\n",
    "    desc_split = desc_lower.str.split(\"|\")\n",
    "    desc_strip = desc_split.apply(lambda x: x[1] if len(x) > 1 else x[0])\n",
    "    # Removing digits and words containing digits\n",
    "    desc_nodigits = desc_strip.apply(lambda x: re.sub(\"\\w*\\d\\w*\", \"\", x))\n",
    "    # Removing punctuation\n",
    "    desc_nopunc = desc_nodigits.apply(lambda x: re.sub(r\"[^\\w\\s]\", \"\", x))\n",
    "    # Removing additional whitespace\n",
    "    desc_clean = desc_nopunc.apply(lambda x: re.sub(' +', ' ', x))\n",
    "    return desc_clean\n",
    "\n",
    "\n",
    "def goodCode(name,code,desc):\n",
    "    A = {'success' : 'true' , \"Message\" : name ,'ICMS' : code ,\"Description\" : desc}\n",
    "    R = A['ICMS'].split('.')\n",
    "    D = [x.strip() for x  in A['Description'].split('\\\\')]\n",
    "    A['R2'] = R[0]\n",
    "    A['R3'] = R[1]\n",
    "    A['R4'] = R[2]\n",
    "\n",
    "    A['Desc2'] = D[0]\n",
    "    A['Desc3'] = D[1]\n",
    "    A['Desc4'] = D[2]\n",
    "    return A\n",
    "\n",
    "def predict(names,clf):\n",
    "    if type(names) == str:\n",
    "        names = [names]\n",
    "    A = pd.DataFrame()\n",
    "    A['comment_list'] = names\n",
    "    A['comment_list'] = A.comment_list.apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, lst_stopwords=lst_stopwords))\n",
    "    A['comment_list_new'] = clean_text_data(A,'comment_list')\n",
    "    vectorizer = CountVectorizer(max_features=max_features,strip_accents='ascii',vocabulary=accepted_words)\n",
    "    X = vectorizer.fit_transform(A['comment_list_new']).toarray()\n",
    "    if X.sum() == 0:\n",
    "        return [{'success' : 'false' , \"Message\" : names[0] ,'ICMS' : 'XX.XX.XXX' ,\"Description\" : 'No ICMS Code'}]\n",
    "    y_pred = clf.predict(X)\n",
    "    codes = [Cat[x] for x in y_pred]\n",
    "    descs =[icms_dct[c] for c in codes]\n",
    "    answer = [goodCode(names[i],codes[i],descs[i]) for i,_ in enumerate(names)]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': 'true',\n",
       " 'Message': 'Clear site of all signs and equipment (including diversion routes)',\n",
       " 'ICMS': '1.01.060',\n",
       " 'Description': 'Capital Construction Costs \\\\ Demolition, site preparation and formation \\\\ Site surface clearance (clearing, grubbing, topsoil stripping, tree felling, minor earthwork, removal)',\n",
       " 'R2': '1',\n",
       " 'R3': '01',\n",
       " 'R4': '060',\n",
       " 'Desc2': 'Capital Construction Costs',\n",
       " 'Desc3': 'Demolition, site preparation and formation',\n",
       " 'Desc4': 'Site surface clearance (clearing, grubbing, topsoil stripping, tree felling, minor earthwork, removal)'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BoQ_text = 'Clear site of all signs and equipment (including diversion routes)'\n",
    "BoQ_text = 'Geophysical Survey in accordance with drawing XX'\n",
    "BoQ_text = 'Termination of optic fibre cable to XX equipment cabinet Type YY'\n",
    "BoQ_text = 'Power reduction joint of XX mm2 to XX mm2'\n",
    "BoQ_text = 'Take down and remove to tip off Site unlit traffic sign including 4 posts'\n",
    "BoQ_text = 'Galvanised high adherence reinforcing strips acting as soil reinforcement'\n",
    "BoQ_text = 'Installation of wildlife tunnel XX m in length as per diagram XX'\n",
    "BoQ_text = 'Clear site of all signs and equipment (including diversion routes)'\n",
    "\n",
    "predict(BoQ_text,clf)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('work')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8e3f23012204b1b655a61e68bcec502af9aea10795c8cc22548649a458b784e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
